{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Linear Regression\n",
    "\n",
    "In this Jupyter Notebook document, everytime you see `#To be completed`, you have to write a piece of code to advance in the exercise.\n",
    "\n",
    "The objective of this exercise is to compute the model from the following data using least square linear regression.\n",
    "\n",
    "Data:\n",
    "\n",
    "| i | 1 | 2 | 3 | 4 | 5 | 6 | 7 |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| X (Input) | -2 | -1 | 0 | 1 | 2 | 3 | 4 |\n",
    "| Y (Output) | -5 | -3 | -1 | 1 | 3 | 5 | 7 |\n",
    "\n",
    "## Numerical least square resolution\n",
    "\n",
    "The estimated function $\\widehat{f(X)}$ is:\n",
    "\n",
    "$\\widehat{f(X)}=\\widehat{\\alpha}X+\\widehat{\\beta}$\n",
    "\n",
    "$\\widehat{\\alpha}=\\frac{\\sum_{i=1}^{n} X_i y_i - n\\bar{X}\\bar{y}}{\\sum_{i=1}^{n} X_i^2 - n\\bar{X}^2}$\n",
    "\n",
    "$\\widehat{\\beta}=\\bar{y}-\\widehat{\\alpha}\\bar{X}$\n",
    "\n",
    "Go on and calculate the estimated function !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "X=[-2,-1,0,1,2,3,4]\n",
    "y=[-5,-3,-1,1,3,5,7]\n",
    "def listElementwiseMult(lista,listb):\n",
    "    return [a*b for a,b in zip(lista,listb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha computation\n",
    "SumXy=#To be completed\n",
    "SumXsquared=#To be completed\n",
    "meanX=#To be completed\n",
    "meany=#To be completed\n",
    "n=len(X)\n",
    "alpha=#To be completed\n",
    "\n",
    "#Check result here\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta computation\n",
    "\n",
    "beta=#To be completed\n",
    "\n",
    "#Check result here\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify if you can predict the value for 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_function(X):\n",
    "    return #To be completed\n",
    "\n",
    "#Try and see what you can estimate\n",
    "estimate_function(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn resolution\n",
    "\n",
    "Now the goal is to perform the same resolution but to obtain it via the sci-kit learn library.\n",
    "Therefore, we are going to use the `LinearRegression()` module from the library `sklearn.linear_model`.\n",
    "\n",
    "`LinearRegression()` is a Python class that allows you to perform an ordinary least squares Linear Regression.\n",
    "It has some options `LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)`:\n",
    "- *fit_intercept* default value is True. If set to False the intercept value that we call here $\\widehat{\\beta}$ would be 0.\n",
    "- *normalize* default value is False. If set to True, X will be normalized before regression, which means that X will be a vector of norm 1.\n",
    "- *copy_X* default value is True. If set to False, operations are performed on X and might change it.\n",
    "- *n_jobs* default value is None, which actually is translated to 1. If required, you can set a different value which would allocate a number of thread (parallel computing) to the learning process.\n",
    "\n",
    "The model is saved inside `LinearRegression()` and you can calculate the values for the model using the method `.fit(X, y)`.\n",
    "\n",
    "Here `X` represents the training data. It has the size `(n_samples,n_features)`, with `n_samples` the number of sample data and `n_features` the number of different features you are working from. In this first class, the number of features will be just 1 and multiple features will be seen in the next course talking about linear regression with multiple variables. So `n_features`=1.\n",
    "\n",
    "`y` represents the target values according to `X`. It has the size `(n_samples,)`.\n",
    "\n",
    "Now let's try to fit a `LinearRegression()` model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "X=[-2,-1,0,1,2,3,4]\n",
    "y=[-5,-3,-1,1,3,5,7]\n",
    "\n",
    "X_train=#To be completed\n",
    "y_train=#To be completed\n",
    "regr = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values that were estimated from that model can then be accessed using `.coef_` and `.intercept_`.\n",
    "\n",
    "`.coef_` is an array of the estimated coefficients for the linear regression problem. It is of shape `(n_features, )`, so in our case it will be of shape  `(1, )`. ($\\widehat{\\alpha}$)\n",
    "\n",
    "`.intercept_` is a float that gives the independent term of the linear model. ($\\widehat{\\beta}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The estimated equation coefficients\n",
    "print('Estimated equation is: (',regr.coef_[0,0],') X + (',regr.intercept_[0],')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations ! your model has learned.\n",
    "\n",
    "Now let's try to predict some values from that `regr` model that we have computed. Therefore we create some testing values continuing our original serie.\n",
    "\n",
    "`LinearRegression()` has a method called `.predict(test_values)`, that requires some `test_values` that we will provide and allows you to predict the target values `y`.\n",
    "\n",
    "Here `X_test` is an array of the next numbers of the serie X, which will be used for the prediction, `y_test` is an array of the actual values we know from that serie that we want to compare to the predicted values and `y_pred` will be the predicted values from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.array([5,6,7,8]).reshape(-1,1)\n",
    "y_test=np.array([9,11,13,15]).reshape(-1,1)\n",
    "\n",
    "y_pred = #To be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyse the result you can use some metrics from the library `sklearn.metrics`\n",
    "\n",
    "This library gives you access to two values, the `mean_squared_error` which is: $\\frac{1}{n}\\sum_{i=1}^n(y_i-\\widehat{y_i})^2$.\n",
    "\n",
    "And the coefficient of determination ($R^2$ score), which you all know from Excel, here `r2_score`, which is $R^2(y, \\widehat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\widehat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your results\n",
    "\n",
    "This part will show you the result you had with the predicted model as a blue line and the test data in a black scatter plot. It uses the library `matplotlib` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(np.append(X_train,X_test), np.append(y_train,y_test),  color='black', label=\"Test data\")\n",
    "plt.plot(np.append(X_train,X_test), np.append(y_train,y_pred), color='blue', linewidth=3, label=\"Predicted Model\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another example of linear regression\n",
    "\n",
    "We will use the Cardekho dataset.\n",
    "This dataset contains information about used cars listed on www.cardekho.com (an indian website) that are available for selling.\n",
    "The goal here is to find linear regression coefficients between different part of that dataset.\n",
    "\n",
    "columns of that dataset are:\n",
    "- *Car_Name* The Name of the car\n",
    "- *Year* The release year of that car (Year it was bought for the first time)\n",
    "- *Selling_Price* The price it is sold\n",
    "- *Km_driven* The kms driven with the car\n",
    "- *Fuel* The type of fuel: Diesel, Petrol, Liquefied Petroleum Gas (LPG) or Compressed Natural Gas (CNG)\n",
    "- *Seller_type* The type of seller that proposes the car: individual, car dealer or trustmark dealer\n",
    "- *Transmission* Manual or Automatic\n",
    "- *Owner* Number of previous owner\n",
    "- *mileage* Consumption in km per l\n",
    "- *engine* Dimension of the engine\n",
    "- *max_power* Maximum power\n",
    "- *Torque* Torque of the car\n",
    "- *Seats* Number of seats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 16)\n",
    "\n",
    "# LOADING THE DATASET\n",
    "# THE DATASET SHOULD BE ALONG WITH THE NOTEBOOK AND THE PYTHON FILE\n",
    "\n",
    "df = pd.read_csv('Cardekho.csv')\n",
    "df['Selling_price']=0.013*df['Selling_price'] # Change the values from Indian Roupies to Us Dollar\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(df['Km_driven'],df['Selling_price'],  color='black', label=\"Test data\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"km driven\")\n",
    "plt.ylabel(\"Selling price ($)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First exercise\n",
    "\n",
    "The goal is to find the relationship between the selling price of the car and the km it was driven and then to analyze the result.\n",
    "\n",
    "We will try to estimate the price of a car according to the km it has driven and to find the general relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KmDriven_train=#To be completed\n",
    "SellPrice_train=#To be completed\n",
    "\n",
    "regr_car = LinearRegression().fit(KmDriven_train, SellPrice_train)\n",
    "\n",
    "# The estimated equation coefficients\n",
    "print('Estimated equation is: (',regr_car.coef_[0,0],') X + (',regr_car.intercept_[0],')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the results using the tools we know. Is this a good result ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KmDriven_test=np.array(df['Km_driven'][-20:]).reshape(-1,1)\n",
    "SellPrice_test=np.array(df['Selling_price'][-20:]).reshape(-1,1)\n",
    "\n",
    "SellPrice_pred = #To be completed\n",
    "\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(SellPrice_test, SellPrice_pred))\n",
    "\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(SellPrice_test, SellPrice_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That result is really poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(KmDriven_test, SellPrice_pred, color='blue', linewidth=3, label=\"Predicted Model\")\n",
    "plt.scatter(df['Km_driven'],df['Selling_price'],  color='black', label=\"Test data\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"km driven\")\n",
    "plt.ylabel(\"Selling price ($)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data by brand. Try with 'Honda' cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Honda=#To be completed\n",
    "Honda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Honda_KmDriven_train=np.array(Honda['Km_driven'][:-20]).reshape(-1,1)\n",
    "Honda_SellPrice_train=np.array(Honda['Selling_price'][:-20]).reshape(-1,1)\n",
    "\n",
    "regr_Honda = #To be completed\n",
    "\n",
    "# The estimated equation coefficients\n",
    "print('Estimated equation is: (',regr_Honda.coef_[0,0],') X + (',regr_Honda.intercept_[0],')')\n",
    "\n",
    "Honda_KmDriven_test=np.array(Honda['Km_driven'][-20:]).reshape(-1,1)\n",
    "Honda_SellPrice_test=#To be completed\n",
    "\n",
    "Honda_SellPrice_pred = #To be completed\n",
    "\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % #To be completed)\n",
    "\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % #To be completed)\n",
    "\n",
    "plt.plot(Honda_KmDriven_test, Honda_SellPrice_pred, color='blue', linewidth=3, label=\"Predicted Model\")\n",
    "plt.scatter(Honda['Km_driven'],Honda['Selling_price'],  color='black', label=\"Honda test data\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"km driven\")\n",
    "plt.ylabel(\"Selling price ($)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a particular Honda model : Honda City\n",
    "\n",
    "To avoid doing the same thing repeatedly create a function called `CarDataKmPriceRegr` that would do the machine learning job, print the outcome and return the model.\n",
    "That function has two input parameters:\n",
    "- *the dataframe* `df`\n",
    "- *the title of the plotted graph* `title`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Honda_City=#To be completed\n",
    "Honda_City.head()\n",
    "\n",
    "def CarDataKmPriceRegr(df,title):\n",
    "    #To be completed\n",
    "    \n",
    "    \n",
    "CarDataKmPriceRegr(Honda_City,'Honda City')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the different types of fuel and car seats number ? \n",
    "what is the difference when you buy a diesel car and a Petrol car ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a big family and you want to have a car which has at least 6 seats, no more than 50 000 kms driven and that uses Petrol. What would be the price of that car ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CarDataKmPriceRegr(#To be completed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second exercise\n",
    "\n",
    "What is the average km driven per year ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting and Overfitting\n",
    "\n",
    "## Definitions\n",
    "\n",
    "**Underfitting** : Underfitting happens when a machine learning model is not complex enough to accurately capture relationships between a dataset’s features and a target variable. An underfitted model results in problematic or erroneous outcomes on new data, or data that it wasn’t trained on, and often performs poorly even on training data.\n",
    "\n",
    "Using underfitted models for decision-making could be costly for businesses. For example, an underfitted model may suggest that you can always make better sales by spending more on marketing when in fact the model fails to capture a saturation effect (at some point, sales will flatten out no matter how much more you spend on marketing). If your business is relying on that model to determine your marketing budget, you will overspend on marketing.\n",
    "\n",
    "**Overfitting** : Overfitting, the counterpart of underfitting, happens when a machine learning model has become too attuned to the data on which it was trained and therefore loses its applicability to any other dataset. A model is overfitted when it is so specific to the original data that trying to apply it to data collected in the future would result in problematic or erroneous outcomes and therefore less-than-optimal decisions.\n",
    "\n",
    "Overfitting causes the model to misrepresent the data from which it learned. An overfitted model will be less accurate on new, similar data than a model which is more generally fitted, but the overfitted one will appear to have a higher accuracy when you apply it to the training data. With no protection against overfitting, model developers might train and deploy a model they think is highly accurate, when in fact it will underperform in production when given new data.\n",
    "\n",
    "Deploying an overfitted model can cause all kinds of problems. For example, if you think your model is 95% accurate in predicting the likelihood of loan default when in reality it is overfitted and has an accuracy somewhere closer to 60%, applying it to future loan decisions will result in the loss of business that would otherwise have been profitable and will result in more dissatisfied customers.\n",
    "\n",
    " <img src=https://3gp10c1vpy442j63me73gy3s-wpengine.netdna-ssl.com/wp-content/uploads/2018/03/Screen-Shot-2018-03-22-at-11.22.15-AM-e1526498075543.png>\n",
    "(Image from https://www.datarobot.com/)\n",
    "\n",
    "## Example\n",
    "\n",
    "This example demonstrates the problems of underfitting and overfitting and\n",
    "how we can use linear regression with polynomial features to approximate\n",
    "nonlinear functions. The plot shows the function that we want to approximate,\n",
    "which is a part of the cosine function. In addition, the samples from the\n",
    "real function and the approximations of different models are displayed. The\n",
    "models have polynomial features of different degrees. We can see that a\n",
    "linear function (polynomial with degree 1) is not sufficient to fit the\n",
    "training samples. This is called **underfitting**. A polynomial of degree 4\n",
    "approximates the true function almost perfectly. However, for higher degrees\n",
    "the model will **overfit** the training data, i.e. it learns the noise of the\n",
    "training data.\n",
    "\n",
    "**overfitting** and **underfitting** can be evaluated by using\n",
    "cross-validation. We calculate the mean squared error (MSE) on the validation\n",
    "set, the higher, the less likely the model generalizes correctly from the\n",
    "training data.\n",
    "\n",
    "### Functions used for the example\n",
    "\n",
    "`Pipeline` is a function that allows to execute a sequence of transforms and a final estimator. It is designed for advanced machine learning projects. Intermediate steps of the pipeline must be ‘transforms’, that is, objects that have the `fit` and `transform` methods. The final estimator has to have a `fit` method. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. Here it is used to combine the `Polynomial Feature` and the `Linear regression`. Try adding `,verbose=True` at the end of the function call to see what is done during the processing.\n",
    "\n",
    "`PolynomialFeatures` generates polynomial and interaction features. It generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is one dimensional of the form $[X]$, it gives the result as a polynom $[1,X,X^2,...,X^n]$. If an input sample is two dimensional and of the form $[a, b]$, the degree-2 polynomial features are $[1, a, b, a^2, ab, b^2]$ (all the coefficients of the $(a+b)^n$ pair). See two examples below.\n",
    "\n",
    "`cross_val_score` evaluates a score by cross-validation. Here `pipeline` is the model to be evaluated, `X[:, np.newaxis]` is the input dataset, `y` are the target values, `scoring` defines the function to be used to compute the score, `cv` is the amount of time the data is cross-validated. For example, `cv=5` computes the score 5 consecutive times with different splits of the input data each time.\n",
    "\n",
    "`np.newaxis` is used to add one dimension to the array. For example, it changes the shape from one numpy array from (X,) to (X,1). See example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "X = np.arange(3).reshape(3, 1)\n",
    "print(X)\n",
    "poly = PolynomialFeatures(3)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(6).reshape(3, 2)\n",
    "print(X)\n",
    "poly = PolynomialFeatures(3)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.arange(3).reshape(3, )\n",
    "print(a.shape)\n",
    "a=a[:, np.newaxis]\n",
    "print(a.shape)\n",
    "a=a[:, np.newaxis]\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "On the left: The figure shows an underfitted model\n",
    "\n",
    "On the middle : The figure shows a well fitted model\n",
    "\n",
    "On the right : The figure shows an overfitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 30\n",
    "degrees = [1, 4, 15]\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
